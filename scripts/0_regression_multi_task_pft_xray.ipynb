{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b7e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import pydicom\n",
    "import cv2\n",
    "import random\n",
    "import sys\n",
    "from torch import nn\n",
    "\n",
    "sys.path.append(\"/mnt/nas32/forGPU/jegal/Workspace/Work/0_CNN_total_Pytorch_new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e76b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1770/648434751.py:1: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  selected_data_df = pd.read_csv(\"../d02_pp_nz_paap_splited.csv\")\n"
     ]
    }
   ],
   "source": [
    "selected_data_df = pd.read_csv(\"../d02_pp_nz_paap_splited.csv\")\n",
    "fvcf_normal_col_name = \"FVC_norm\"\n",
    "fvcf_measured_col_name = \"FVC_meas\"\n",
    "\n",
    "fevf_normal_col_name = \"FEV_norm\"\n",
    "fevf_measured_col_name = \"FEV_meas\"\n",
    "\n",
    "task_name = \"d02\"\n",
    "get_recon = False\n",
    "\n",
    "log_folder = f\"../results/{task_name}\"\n",
    "if get_recon:\n",
    "    log_folder = f\"{log_folder}_recon\"\n",
    "    \n",
    "log_csv_path = f\"{log_folder}/log.csv\"\n",
    "log_plot_folder = f\"{log_folder}/plots\"\n",
    "log_weight_folder = f\"{log_folder}/weights\"\n",
    "os.makedirs(log_plot_folder, exist_ok=True)\n",
    "os.makedirs(log_weight_folder, exist_ok=True)\n",
    "\n",
    "num_epochs = 100\n",
    "num_gpu = torch.cuda.device_count()\n",
    "batch_size = 8 * num_gpu\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "DEVICE = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14b1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_array_with_rescale(dicom_path):\n",
    "    \"\"\"\n",
    "    Load a DICOM file and return the rescaled pixel array.\n",
    "    Applies Rescale Intercept and Rescale Slope if available.\n",
    "    \n",
    "    Parameters:\n",
    "        dicom_path (str): Path to the DICOM file.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Rescaled pixel array.\n",
    "    \"\"\"\n",
    "    dicom_data = pydicom.dcmread(dicom_path, force=True)\n",
    "    pixel_array = dicom_data.pixel_array\n",
    "    intercept = getattr(dicom_data, \"RescaleIntercept\", 0)\n",
    "    slope = getattr(dicom_data, \"RescaleSlope\", 1)\n",
    "    return pixel_array * slope + intercept\n",
    "\n",
    "class PFTDataset(Dataset):\n",
    "    def __init__(self, dcm_path_list, fvcf_array, fevf_array):\n",
    "        assert len(dcm_path_list) == len(fvcf_array)\n",
    "        assert len(fvcf_array) == len(fevf_array)\n",
    "        \n",
    "        self.dcm_path_list = dcm_path_list\n",
    "        self.fvcf_array = fvcf_array\n",
    "        self.fevf_array = fevf_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dcm_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        dcm_path = self.dcm_path_list[idx]\n",
    "        fvcf_value = self.fvcf_array[idx]\n",
    "        fevf_value = self.fevf_array[idx]\n",
    "        # xray_array.shape = [1, 512, 512]\n",
    "        xray_array = load_dicom_array_with_rescale(dcm_path)\n",
    "        xray_array = cv2.resize(xray_array, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        xray_array = (xray_array - xray_array.mean()) / xray_array.std()\n",
    "        xray_array = torch.tensor(xray_array[None], dtype=torch.float32)\n",
    "        pft_value = torch.tensor([fvcf_value, fevf_value], dtype=torch.float32)\n",
    "        return xray_array, pft_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bb3d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dcm_path(dcm_path):\n",
    "    dcm_path = dcm_path.replace(\"/workspace/nas216\", \"/mnt/nas216\")\n",
    "    return dcm_path\n",
    "\n",
    "def get_phase_dcm_path_list(selected_data_df, phase_str):\n",
    "    phase_row = selected_data_df[\"phase\"] == phase_str\n",
    "    phase_df = selected_data_df[phase_row]\n",
    "    phase_dcm_path_list = list(phase_df[\"dcm_path\"])\n",
    "    phase_dcm_path_list = [convert_dcm_path(dcm_path) for dcm_path in phase_dcm_path_list]\n",
    "    return phase_dcm_path_list\n",
    "\n",
    "def get_pft_value_list(selected_data_df, phase_str):\n",
    "    phase_row = selected_data_df[\"phase\"] == phase_str\n",
    "    phase_df = selected_data_df[phase_row]\n",
    "    \n",
    "    phase_fvcf_normal_col_array = np.array(phase_df[fvcf_normal_col_name])\n",
    "    phase_fvcf_measured_col_array = np.array(phase_df[fvcf_measured_col_name])\n",
    "    \n",
    "    phase_fevf_normal_col_array = np.array(phase_df[fevf_normal_col_name])\n",
    "    phase_fevf_measured_col_array = np.array(phase_df[fevf_measured_col_name])\n",
    "    \n",
    "    fvcf_array = (phase_fvcf_measured_col_array - phase_fvcf_normal_col_array) / phase_fvcf_normal_col_array\n",
    "    fevf_array = (phase_fevf_measured_col_array - phase_fevf_normal_col_array) / phase_fevf_normal_col_array\n",
    "    return fvcf_array, fevf_array\n",
    "\n",
    "train_dcm_path_list = get_phase_dcm_path_list(selected_data_df, \"train\")\n",
    "val_dcm_path_list = get_phase_dcm_path_list(selected_data_df, \"val\")\n",
    "test_dcm_path_list = get_phase_dcm_path_list(selected_data_df, \"test\")\n",
    "\n",
    "train_fvcf_array, train_fevf_array = get_pft_value_list(selected_data_df, \"train\")\n",
    "val_fvcf_array, val_fevf_array = get_pft_value_list(selected_data_df, \"val\")\n",
    "test_fvcf_array, test_fevf_array = get_pft_value_list(selected_data_df, \"test\")\n",
    "\n",
    "train_dataset = PFTDataset(train_dcm_path_list, train_fvcf_array, train_fevf_array)\n",
    "val_dataset = PFTDataset(val_dcm_path_list, val_fvcf_array, val_fevf_array)\n",
    "test_dataset = PFTDataset(test_dcm_path_list, test_fvcf_array, test_fevf_array)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, shuffle=False, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a03dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512]) torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "from src.model.inception_resnet_v2.multi_task.multi_task_2d_v2 import InceptionResNetV2MultiTask2D\n",
    "from src.model.inception_resnet_v2.diffusion.diff_ae.diffusion_layer import GroupNorm32\n",
    "model = InceptionResNetV2MultiTask2D(input_shape=(1, 512, 512), class_channel=2, seg_channels=None, validity_shape=(1, 8, 8), inject_class_channel=None,\n",
    "                                     block_size=16, include_cbam=False, decode_init_channel=None,\n",
    "                                     norm=\"group\", act=\"silu\", dropout_proba=0.05,\n",
    "                                     seg_act=\"softmax\", class_act=\"tanh\", recon_act=None, validity_act=\"sigmoid\",\n",
    "                                     get_seg=False, get_class=True, get_recon=get_recon, get_validity=False,\n",
    "                                     use_class_head_simple=True, include_upsample=False,\n",
    "                                     use_decode_simpleoutput=True, use_seg_conv_transpose=True,\n",
    "                                     use_checkpoint=False).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    xray_array, pft_value = train_dataset[0]\n",
    "    xray_array, pft_value = xray_array[None].to(DEVICE), pft_value[None].to(DEVICE)\n",
    "    print(xray_array.shape, pft_value.shape)\n",
    "    if get_recon:\n",
    "        pred_pft_value, pred_xray_array = model(xray_array)\n",
    "        print(pred_xray_array.shape, pred_pft_value.shape)\n",
    "    else:\n",
    "        pred_pft_value = model(xray_array)\n",
    "        print(pred_pft_value.shape)\n",
    "        \n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880432da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/d02/log.csv check exist...\n",
      "../results/d02/log.csv exist.\n",
      "../results/d02/log.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "from src.model.train_util.logger import CSVLogger\n",
    "from src.model.train_util.scheduler import OneCycleLR\n",
    "\n",
    "epoch_col = [\"epoch\"]\n",
    "train_col = [\"train_loss\", \"pft_l1_loss\"]\n",
    "val_col = [\"val_loss\", \"val_pft_l1_loss\"]\n",
    "if get_recon:\n",
    "    train_col.append(\"recon_l1_loss\")\n",
    "    val_col.append(\"val_recon_l1_loss\")\n",
    "\n",
    "csv_logger = CSVLogger(log_csv_path, epoch_col + train_col + val_col)\n",
    "loss_fn = F.l1_loss\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "step_size = len(train_dataloader)  # 매 30 step마다\n",
    "\n",
    "# StepLR 스케줄러 정의\n",
    "scheduler_params = {\n",
    "\"step_size\": step_size,\n",
    "\"first_epoch\": 2,\n",
    "\"second_epoch\": 68,\n",
    "\"total_epoch\": num_epochs\n",
    "}\n",
    "scheduler = OneCycleLR(optimizer, **scheduler_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5115f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▊                                                                                                                           | 71/3252 [02:01<1:30:29,  1.71s/it, loss_mean=0.1300, loss_current=0.1537]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 7.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1770/2366529230.py\", line 36, in __getitem__\n    xray_array = load_dicom_array_with_rescale(dcm_path)\n  File \"/tmp/ipykernel_1770/2366529230.py\", line 12, in load_dicom_array_with_rescale\n    dicom_data = pydicom.dcmread(dicom_path, force=True)\n  File \"/opt/conda/lib/python3.10/site-packages/pydicom/filereader.py\", line 1042, in dcmread\n    fp = open(fp, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/nas216/ds_pft_cxr/data/d02/CR09477/1/172_2023-0481_CR09477.1.1.dcm'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m val_pft_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m val_recon_loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xray_array, pft_value \u001b[38;5;129;01min\u001b[39;00m train_pbar:\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     xray_array, pft_value \u001b[38;5;241m=\u001b[39m xray_array\u001b[38;5;241m.\u001b[39mto(DEVICE), pft_value\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 7.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1770/2366529230.py\", line 36, in __getitem__\n    xray_array = load_dicom_array_with_rescale(dcm_path)\n  File \"/tmp/ipykernel_1770/2366529230.py\", line 12, in load_dicom_array_with_rescale\n    dicom_data = pydicom.dcmread(dicom_path, force=True)\n  File \"/opt/conda/lib/python3.10/site-packages/pydicom/filereader.py\", line 1042, in dcmread\n    fp = open(fp, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/mnt/nas216/ds_pft_cxr/data/d02/CR09477/1/172_2023-0481_CR09477.1.1.dcm'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_pft_loss_list = []\n",
    "    train_recon_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_pft_loss_list = []\n",
    "    val_recon_loss_list = []\n",
    "    \n",
    "    for xray_array, pft_value in train_pbar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        xray_array, pft_value = xray_array.to(DEVICE), pft_value.to(DEVICE)\n",
    "        if get_recon:\n",
    "            pred_pft_value, pred_xray_array = model(xray_array)\n",
    "            recon_loss = loss_fn(pred_xray_array, xray_array)\n",
    "        else:\n",
    "            pred_pft_value = model(xray_array)\n",
    "            recon_loss = torch.tensor(0)\n",
    "        pft_loss = loss_fn(pred_pft_value, pft_value)\n",
    "        \n",
    "        loss = pft_loss * 0.9 + recon_loss * 0.1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss_list.append(loss.item())\n",
    "        train_pft_loss_list.append(pft_loss.item())\n",
    "        train_recon_loss_list.append(recon_loss.item())\n",
    "        train_pbar.set_postfix({'loss_mean': f\"{np.mean(train_loss_list):.4f}\", 'loss_current': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xray_array, pft_value in val_dataloader:\n",
    "            xray_array, pft_value = xray_array.to(DEVICE), pft_value.to(DEVICE)\n",
    "            if get_recon:\n",
    "                pred_pft_value, pred_xray_array = model(xray_array)\n",
    "                recon_loss = loss_fn(pred_xray_array, xray_array)\n",
    "            else:\n",
    "                pred_pft_value = model(xray_array)\n",
    "                recon_loss = torch.tensor(0)\n",
    "            pft_loss = loss_fn(pred_pft_value, pft_value)\n",
    "\n",
    "            loss = pft_loss * 0.9 + recon_loss * 0.1\n",
    "\n",
    "            val_loss_list.append(loss.item())\n",
    "            val_pft_loss_list.append(pft_loss.item())\n",
    "            val_recon_loss_list.append(recon_loss.item())\n",
    "    \n",
    "    ######################## Write Csv row #####################\n",
    "    epoch_row_value_list = [f\"{epoch}\"]\n",
    "    train_row_value_list = [f\"{np.mean(train_loss_list):.3f}\", \n",
    "                            f\"{np.mean(train_pft_loss_list):.3f}\",\n",
    "                            f\"{np.mean(train_recon_loss_list):.3f}\"]\n",
    "    val_row_value_list = [f\"{np.mean(val_loss_list):.3f}\", \n",
    "                            f\"{np.mean(val_pft_loss_list):.3f}\",\n",
    "                            f\"{np.mean(val_recon_loss_list):.3f}\"]\n",
    "    \n",
    "    row_value_list = epoch_row_value_list + train_row_value_list + val_row_value_list\n",
    "    csv_logger.writerow(row_value_list)\n",
    "    \n",
    "    ######################## Save model ########################\n",
    "    torch.save({\n",
    "    \"model\": diffusion_model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    },\n",
    "    f\"./{log_weight_folder}/{epoch:03d}.ckpt\")\n",
    "    ######################## Plot sample ########################\n",
    "    with torch.no_grad():\n",
    "        xray_array, pft_value = random.choice(val_dataset)\n",
    "        xray_array, pft_value = xray_array[None].to(DEVICE), pft_value[None].to(DEVICE)\n",
    "        if get_recon:\n",
    "            pred_pft_value, pred_xray_array = model(xray_array)\n",
    "        else:\n",
    "            pred_pft_value = model(xray_array)\n",
    "            \n",
    "        pft_value_list = list(pft_value.cpu().numpy().round(3)[0])\n",
    "        pred_pft_value_list = list(pred_pft_value.cpu().numpy().round(3)[0])\n",
    "        _, ax = plt.imshow(1, 1, figsize=(8, 8))\n",
    "        ax.imshow(xray_array, cmap=\"gray\")\n",
    "        ax.set_title(f\"GT: {pft_value_list}, PRED: {pred_pft_value_list}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"./{log_plot_folder}/polt_{epoch:03d}.png\")\n",
    "        plt.clf()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657e0685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a43629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
