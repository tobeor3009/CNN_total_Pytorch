{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pytorch-lightning in /home/jegal/.local/lib/python3.6/site-packages (1.5.9)\n",
            "Requirement already satisfied: torch>=1.7.* in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (1.10.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (2022.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.5.0)\n",
            "Requirement already satisfied: pyDeprecate==0.3.1 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (59.5.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (20.9)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /home/jegal/.local/lib/python3.6/site-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: aiohttp in /home/jegal/.local/lib/python3.6/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.30.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.0.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.*->pytorch-lightning) (0.8)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: idna-ssl>=1.0 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/jegal/.local/lib/python3.6/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\n",
            "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_array = np.ones((32,512,512))\n",
        "slice_range = np.arange(img_array.shape[0] - 31)\n",
        "slice_range = np.random.choice(slice_range)\n",
        "img_array = img_array[slice_range:slice_range+32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./datasets/1.normal_npy//train/0.69_0/ct_series_1752090475_2.npy\n",
            "(512, 512, 30)\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "path_list = glob(f\"{data_common_path}/train/*/*.npy\")\n",
        "for train_image_path in path_list:\n",
        "    img_array = np.load(train_image_path)\n",
        "    if img_array.shape[2] < 32:\n",
        "        print(train_image_path)\n",
        "        print(img_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "path_list = glob(f\"{data_common_path}/valid/*/*.npy\")\n",
        "for train_image_path in path_list:\n",
        "    img_array = np.load(train_image_path)\n",
        "    if img_array.shape[2] < 32:\n",
        "        print(train_image_path)\n",
        "        print(img_array.shape)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "path_list = glob(f\"{data_common_path}/test/*/*.npy\")\n",
        "for train_image_path in path_list:\n",
        "    img_array = np.load(train_image_path)\n",
        "    if img_array.shape[2] < 32:\n",
        "        print(train_image_path)\n",
        "        print(img_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "gpu_number = \"2\"\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_number\n",
        "\n",
        "def windowing_brain(img_array, channel=3, return_uint8=True):\n",
        "    img_array = img_array.transpose((2, 0, 1))\n",
        "#     slice_range = np.arange(img_array.shape[0])\n",
        "#     slice_range = np.random.choice(slice_range, 32)\n",
        "#     slice_range = np.sort(slice_range)\n",
        "#     img_array = img_array[slice_range]\n",
        "    slice_range = np.arange(img_array.shape[0] - 31)\n",
        "    slice_range = np.random.choice(slice_range)\n",
        "    img_array = img_array[slice_range:slice_range+32]\n",
        "    if channel == 1:\n",
        "        img_array = img_array + 40\n",
        "        img_array = img_array + 40\n",
        "        img_array = np.clip(img_array, 0, 160)\n",
        "        img_array = img_array / 160\n",
        "\n",
        "    elif channel == 3:\n",
        "        dcm0 = img_array - 5\n",
        "        dcm0 = np.clip(dcm0, 0, 50)\n",
        "        dcm0 = dcm0 / 50.\n",
        "\n",
        "        dcm1 = img_array + 0\n",
        "        dcm1 = np.clip(dcm1, 0, 80)\n",
        "        dcm1 = dcm1 / 80.\n",
        "\n",
        "        dcm2 = img_array + 20\n",
        "        dcm2 = np.clip(dcm2, 0, 200)\n",
        "        dcm2 = dcm2 / 200.\n",
        "\n",
        "        img_array = np.stack([dcm0, dcm1, dcm2], 0)\n",
        "        \n",
        "    if return_uint8: \n",
        "        return np.uint8(img_array * (2 ** 8 - 1))\n",
        "    \n",
        "    else: # the value is normalized to [0, 1]\n",
        "        return img_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total data num 20161\n",
            "Total data num 2520\n",
            "Total data num 2442\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from src.data_loader.classification import ClassifyDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "\n",
        "batch_size = 2\n",
        "on_memory = False\n",
        "augmentation_proba = 0.8\n",
        "augmentation_policy_dict = {\n",
        "    \"positional\": True,\n",
        "    \"noise\": False,\n",
        "    \"elastic\":False,\n",
        "    \"brightness_contrast\": False,\n",
        "    \"color\": False,\n",
        "    \"to_jpeg\": False\n",
        "}\n",
        "image_channel_dict={\"image\": \"rgb\"}\n",
        "preprocess_input = windowing_brain\n",
        "target_size = (512, 512)\n",
        "interpolation = \"bilinear\"\n",
        "class_mode = \"binary\"\n",
        "# class_mode = \"categorical\"\n",
        "dtype=\"float32\"\n",
        "\n",
        "data_common_path = \"./datasets/1.normal_npy/\"\n",
        "\n",
        "train_image_path_list = glob(f\"{data_common_path}/train/*/*.npy\")\n",
        "valid_image_path_list = glob(f\"{data_common_path}/valid/*/*.npy\")\n",
        "test_image_path_list = glob(f\"{data_common_path}/test/*/*.npy\")\n",
        "\n",
        "label_list = os.listdir(f\"{data_common_path}/train/\")\n",
        "\n",
        "label_to_index_dict = {label:index for index, label in enumerate(label_list)}\n",
        "index_to_label_dict = {index:label for index, label in enumerate(label_list)}\n",
        "\n",
        "label_policy = lambda label: label_to_index_dict[label]\n",
        "def label_policy(label):\n",
        "    age, gender = label.split(\"_\")\n",
        "    age, gender = float(age), float(gender)\n",
        "    return [age, gender]\n",
        "\n",
        "common_arg_dict = {\n",
        "    \"label_policy\": label_policy,\n",
        "    \"augmentation_policy_dict\": augmentation_policy_dict,\n",
        "    \"image_channel_dict\": image_channel_dict,\n",
        "    \"preprocess_input\": preprocess_input,\n",
        "    \"target_size\": target_size,\n",
        "    \"interpolation\": interpolation,\n",
        "    \"class_mode\": class_mode,\n",
        "    \"dtype\": dtype\n",
        "}\n",
        "\n",
        "num_workers = min(batch_size // 2, 8)\n",
        "\n",
        "train_dataset = ClassifyDataset(image_path_list=train_image_path_list,\n",
        "                               on_memory=on_memory,\n",
        "                               augmentation_proba=augmentation_proba,\n",
        "                                **common_arg_dict\n",
        ")\n",
        "valid_dataset = ClassifyDataset(image_path_list=valid_image_path_list,\n",
        "                               on_memory=on_memory,\n",
        "                               augmentation_proba=0,\n",
        "                                **common_arg_dict\n",
        ")\n",
        "test_dataset = ClassifyDataset(image_path_list=test_image_path_list,\n",
        "                               on_memory=False,\n",
        "                               augmentation_proba=0,\n",
        "                               **common_arg_dict\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# F.cross_entropy\n",
        "# FM.accuracy\n",
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self, model, optimizer, loss_fun, metric, lr):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fun = loss_fun\n",
        "        self.metric = metric\n",
        "        self.lr = lr\n",
        "        self.act_layer = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        output = self.act_layer(output)\n",
        "        return output\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fun(logits, y)\n",
        "        age_average_precision, genender_acc = self.metric(logits, y)\n",
        "        metrics = {'train_precision': age_average_precision, 'train_acc': genender_acc, 'train_loss': loss}\n",
        "        self.log_dict(metrics, on_step=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fun(logits, y)\n",
        "        age_average_precision, genender_acc = self.metric(logits, y)\n",
        "        metrics = {'val_precision': age_average_precision,'val_acc': genender_acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fun(logits, y)\n",
        "        age_average_precision, genender_acc = self.metric(logits, y)\n",
        "        metrics = {'test_precision': age_average_precision, 'test_acc': genender_acc, 'test_loss': loss}\n",
        "        self.log_dict(metrics)\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer(self.model.parameters(), lr=self.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------------\n",
            "           Layer (type)              Input Shape         Param #     Tr. Param #\n",
            "=================================================================================\n",
            "    InceptionResNetV2-1     [1, 3, 32, 512, 512]      73,098,464      73,098,464\n",
            "    AdaptiveAvgPool3d-2     [1, 1536, 2, 14, 14]               0               0\n",
            "               Linear-3            [1, 98, 1536]         786,944         786,944\n",
            "   PositionalEncoding-4             [1, 98, 512]               0               0\n",
            "   TransformerEncoder-5             [1, 98, 512]       6,712,704       6,712,704\n",
            "               Linear-6                 [1, 512]         131,328         131,328\n",
            "              Dropout-7                 [1, 256]               0               0\n",
            "                ReLU6-8                 [1, 256]               0               0\n",
            "               Linear-9                 [1, 256]          32,896          32,896\n",
            "             Dropout-10                 [1, 128]               0               0\n",
            "               ReLU6-11                 [1, 128]               0               0\n",
            "              Linear-12                 [1, 128]             258             258\n",
            "             Sigmoid-13                   [1, 2]               0               0\n",
            "=================================================================================\n",
            "Total params: 80,762,594\n",
            "Trainable params: 80,762,594\n",
            "Non-trainable params: 0\n",
            "---------------------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from src.model.inception_resnet_v2.classification import InceptionResNetV2Transformer3D\n",
        "import torch\n",
        "import pytorch_model_summary\n",
        "\n",
        "base_model = InceptionResNetV2Transformer3D(n_input_channels=3, block_size=16,\n",
        "                                             padding='valid', z_channel_preserve=True,\n",
        "                                             dropout_proba=0, num_class=2,\n",
        "                                             include_context=False, use_base=True).cuda()\n",
        "print(pytorch_model_summary.summary(base_model, torch.zeros(1, 3, 32, 512, 512).float().cuda(), show_input=True))\n",
        "base_model(torch.zeros(1, 3, 32, 512, 512).cuda()).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from timm.models import nf_resnet101 \n",
        "import torchmetrics\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam\n",
        "\n",
        "def loss_fun(logit, y):\n",
        "    logit_age = logit[..., 0]\n",
        "    y_age = y[0]\n",
        "    logit_gender = logit[..., 1]\n",
        "    y_gender = y[1].float()\n",
        "    age_loss = nn.L1Loss().cuda()(logit_age, y_age)\n",
        "    gender_loss = nn.BCELoss().cuda()(logit_gender, y_gender)\n",
        "        \n",
        "    return age_loss + gender_loss\n",
        "\n",
        "def metric(logit, y):\n",
        "    logit_age = logit[..., 0]\n",
        "    y_age = y[0]\n",
        "    logit_gender = logit[..., 1]\n",
        "    y_gender = y[1].int()\n",
        "    \n",
        "    age_average_precision = torchmetrics.MeanAbsoluteError().cuda()(logit_age, y_age)\n",
        "    genender_acc = torchmetrics.Accuracy().cuda()(logit_gender, y_gender)\n",
        "    \n",
        "    return age_average_precision, genender_acc\n",
        "\n",
        "logger = CSVLogger(\"/home/jegal/mount/pathology31/jegal/Workspace/Data/CT/이승준_Brain_나이_성별/logs\", name=\"brain_classification_3d\")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='/home/jegal/mount/pathology31/jegal/Workspace/Data/CT/이승준_Brain_나이_성별/logs/',\n",
        "    filename='epoch{epoch:02d}-val_loss{val_loss:.2f}',\n",
        "    auto_insert_metric_name=False,\n",
        "    save_top_k=5, \n",
        ")\n",
        "\n",
        "model = Classifier(base_model, optimizer, loss_fun, metric, 1e-4).cuda()\n",
        "# model = Classifier.load_from_checkpoint(checkpoint_path=\"./logs/epoch03-val_loss0.14.ckpt\",\n",
        "#                                                                                  model=base_model,\n",
        "#                                                                                  optimizer=optimizer,\n",
        "#                                                                                  loss_fun=loss_fun,\n",
        "#                                                                                  metric=metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n",
            "\n",
            "  | Name      | Type                           | Params\n",
            "-------------------------------------------------------------\n",
            "0 | model     | InceptionResNetV2Transformer3D | 80.8 M\n",
            "1 | act_layer | Sigmoid                        | 0     \n",
            "-------------------------------------------------------------\n",
            "80.8 M    Trainable params\n",
            "0         Non-trainable params\n",
            "80.8 M    Total params\n",
            "323.050   Total estimated model params size (MB)\n",
            "/home/jegal/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /home/jegal/mount/pathology31/jegal/Workspace/Data/CT/이승준_Brain_나이_성별/logs exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jegal/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` augment` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
            "/home/jegal/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` augment` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e233aaab9e45f792432664047d3bf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jegal/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(gpus=1, logger=logger, callbacks=[checkpoint_callback])\n",
        "trainer.fit(model, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline 성능\n",
        "- sex accuracy: 98 %\n",
        "- age: L1 distance -> age < 3 years\n",
        "    - age를 잘 맞추는 게 임상적으로 더 의미 있음.\n",
        "    \n",
        "- modeling\n",
        "    - Conv3D vs. CNN + Transformer\n",
        "        - Conv3D: 학습했을 때 3 살 내로 된다면?? -> 심플하니까\n",
        "        - 만약 잘못한다 -> CNN + Transformer\n",
        "- Augmentations\n",
        "    - geometric: flip, rotation \n",
        "    - blur, ...?\n",
        "    \n",
        "- 32 장 기준으로 만들기\n",
        "    - 32장 \n",
        "    - 32장 이상 -> 일정한 간격으로 32장 골라 쓰기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unused Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model.inception_resnet_v2.classification import InceptionResNetV2Transformer3D\n",
        "import torch\n",
        "import pytorch_model_summary\n",
        "\n",
        "base_model = InceptionResNetV2Transformer3D(n_input_channels=3, block_size=16,\n",
        "                                             padding='valid', z_channel_preserve=True,\n",
        "                                             dropout_proba=0.3, num_class=2,\n",
        "                                             include_context=False, use_base=True).cuda()\n",
        "print(pytorch_model_summary.summary(base_model, torch.zeros(1, 3, 32, 512, 512).float().cuda(), show_input=True))\n",
        "base_model(torch.zeros(1, 3, 32, 512, 512).cuda()).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model.inception_resnet_v2.classification import InceptionResNetV2Transformer3D\n",
        "import torch\n",
        "import pytorch_model_summary\n",
        "\n",
        "base_model = InceptionResNetV2Transformer3D(n_input_channels=3, block_size=16,\n",
        "                                             padding='valid', z_channel_preserve=True,\n",
        "                                             dropout_proba=0.3, num_class=2,\n",
        "                                             include_context=False, use_base=True).cuda()\n",
        "print(pytorch_model_summary.summary(base_model, torch.zeros(1, 3, 32, 512, 512).float().cuda(), show_input=True))\n",
        "base_model(torch.zeros(1, 3, 32, 512, 512).cuda()).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import shutil\n",
        "data_common_path = \"./datasets/1.normal_npy/\"\n",
        "os.makedirs(f\"{data_common_path}/train\", exist_ok=True)\n",
        "os.makedirs(f\"{data_common_path}/valid\", exist_ok=True)\n",
        "os.makedirs(f\"{data_common_path}/test\", exist_ok=True)\n",
        "\n",
        "data_path = glob(f\"{data_common_path}/*\")\n",
        "\n",
        "data_path_list = glob(f\"{data_common_path}/*\")[:-3]\n",
        "\n",
        "for data_path in tqdm(data_path_list):\n",
        "    source = os.path.basename(data_path)\n",
        "    npy_path_list = glob(f\"{data_path}/*.npy\") \n",
        "    random.shuffle(npy_path_list)\n",
        "    npy_num = len(npy_path_list)\n",
        "    train_num = math.ceil(npy_num * 0.8)\n",
        "    valid_num = math.ceil(npy_num * 0.9)\n",
        "    for npy_index, npy_path in enumerate(npy_path_list):\n",
        "        npy_basename = os.path.basename(npy_path)\n",
        "        if npy_index < train_num:\n",
        "            target = \"train\"\n",
        "        elif npy_index < valid_num:\n",
        "            target = \"valid\"\n",
        "        else:\n",
        "            target = \"test\"\n",
        "            \n",
        "        os.makedirs(f\"{data_common_path}/{target}/{source}\", exist_ok=True)    \n",
        "        new_npy_path = f\"{data_common_path}/{target}/{source}/{npy_basename}\"\n",
        "        shutil.move(npy_path, new_npy_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(32, 512, 512)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = np.zeros((48,512,512))\n",
        "slice_range = np.arange(48)\n",
        "slice_range = np.random.choice(slice_range, 32)\n",
        "slice_range = np.sort(slice_range)\n",
        "\n",
        "temp[slice_range].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slice_range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model.net_3d.resnet import ResNet, BasicBlock, Bottleneck\n",
        "import torch\n",
        "import pytorch_model_summary\n",
        "\n",
        "base_model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512], n_input_channels=3, n_classes=2).cuda()\n",
        "print(pytorch_model_summary.summary(base_model, torch.zeros(1, 3, 32, 512, 512).cuda(), show_input=True))\n",
        "# base_model(torch.zeros(1, 1, 32, 512, 256).cuda).shape"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}